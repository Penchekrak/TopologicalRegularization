%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}

\paragraph{Preface.}
Generative neural network models nowadays are used to create all kinds of data -- images, 3D assets, video sequences, other samples from various distributions with limited known properties (for example distributions of network weights) -- the possible data types are not limited to those, but we will be focusing on them. There are numerous approaches to create such models, among which the more effective being diffusion-/adversarial-based training and semi-implicit probabilistic models like Autoencoders and Normalizing Flows. Appropriate probabilistic setups in above-mentioned approaches result in specific loss formulation for the training procedure, e.g. adversarial loss for GAN training, or reconstruction loss for Autoencoders and diffusion models. However, to either enhance the quality of resulting model or to stabilize training or for many other reasons such loss functions are adjusted with regularization and other auxiliary terms. Weight regularization and perceptual loss are being common examples. There is group of additional loss terms that exist to force generative samples to satisfy some constrains, this includes, for example, MSE between Sobel-filtered images to ensure boundary preservation, cycle-consistency loss for content-preserving image transformations, etc. But this is not limited to per-object objectives. There are distribution divergences, contrastive losses and other objectives that restrict "global" generative behaviour of the model.

In the field of Computer Vision generative models are of particular and very high interest. Many methods, producing overwhelming, state of the art results are created. There are, however, several limitations for those methods, that are not yet overcome. In this work we are aimed towards solving some of those issues with the help of topology-aware objectives, such as manifold-topology-divergence \cite{MTopDiv} and Wasserstein distance between persistent diagrams \cite{topo-segm}. Examples of such methods and their possible problems are: 3D scene/object generation via NERF or Gaussian Splats prediction, where spurious connected components occur, lowering quality of generation and requiring manual removal; Generation of images of structured objects that have certain semantics behind their real-life topology, e.g. satellite photos of roads, which need to preserve such topology upon generation and restoration; Modalities of modeled distributions are also oftentimes corrupt (so called mode-collapse or mode-invention), which is itself a somewhat topological phenomenon in nature.

\paragraph{Contribution.}
This work studies an application of a particular class of loss regularizers that are defined by means of topological quantities, namely persistent barcodes for zero-dimensional homologies, to some of problems outlined above. First major contribution is an application of manifold-topology-divergence \cite{MTopDiv} as an auxiliary loss term for generative modeling, which improved mode coverage and overall sample quality. Second contribution is usage of similar loss function to reduce occurrence of disconnected clusters (floaters) in 3D point clouds and formulation of simple and interpretable metric to assess presence of such floaters.

\paragraph{Practical value.}
    \begin{itemize}
        \item Most modern methods of generative modeling (such as diffusion models \cite{ho2020denoising} and autoregressive decoders \cite{tian2024visual}) are highly resource demanding during inference, which renders them difficult do deploy on edge devices and employ for creative tasks which require large numbers of generative samples, such as long video sequences or high definition image synthesis. GANs on the other hand do not suffer from this limitation, and our method may enable GANs to also display high diversity of samples bringing them closer to SOTA generative methods.
        \item Superfluous points and floaters in 3D reconstruction worsen quality of point cloud representation, requiring end user to manually modify output of reconstruction algorithms, which harms automation and wide adoption of the processes such as Gaussian Splatting \cite{kerbl20233d}. Proposed regularization reduces number of floaters and in turn, lessens required manual labor.
    \end{itemize}
