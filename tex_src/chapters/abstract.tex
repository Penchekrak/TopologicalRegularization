This thesis work focuses on studying several applications of topological regularization (loss) to practical problems of training neural networks. Such objective may be imposed on structures within an objects of training set or on a large sample of objects as a whole, because data almost always have relational structure. We examine the latter idea thoroughly, via experimenting with generative models and outlining the applicability in the first case. Specifically we assess how generative capabilities of Wasserstein GAN change if trained with said topological loss for toy two dimensional datasets and standard vision datasets: MNIST and CelebA. As an addition we apply topological regularization to optimization procedure for 3d Gaussian Splatting reconstruction, thus reducing number of unwanted connected components in underlying point cloud. We also propose simplistic and interpretable metric to measure the effect of our regularization.
